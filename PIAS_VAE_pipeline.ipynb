{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701d5825",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # PIAS-VAE End-to-End Pipeline\n",
    "# This notebook runs:\n",
    "# 1) Train LSTM-VAE on NORMAL sequences\n",
    "# 2) Create synthetic anomalies using interpolation + SMOTE-style latent perturbation\n",
    "# 3) Train LSTM classifier on augmented dataset (best model saved by anomaly F1)\n",
    "# 4) Threshold calibration for anomaly detection\n",
    "# 5) Evaluation and artifact saving\n",
    "#\n",
    "# Requirements:\n",
    "# - torch, numpy, pandas, sklearn, matplotlib, seaborn\n",
    "# - The file `src/vae_time_series.py` (the LSTM VAE) should be present (we used your provided version)\n",
    "# - This notebook will generate mock CMAPSS-like data if real data is not present\n",
    "#\n",
    "# Tweak constants below as needed.\n",
    "\n",
    "# %%\n",
    "# Imports & constants\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, precision_recall_fscore_support, confusion_matrix\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Make sure your src module is importable\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "# Import VAE helpers from your src module\n",
    "from vae_time_series import PIASVAE, train_vae_from_dataloader, synthesize_and_save, \\\n",
    "    encode_dataset_to_latents, decode_latents_to_sequences, SEQUENCE_LENGTH, NUM_FEATURES, LATENT_DIM\n",
    "\n",
    "# Directory layout\n",
    "os.makedirs('data/01_raw', exist_ok=True)\n",
    "os.makedirs('data/02_interim', exist_ok=True)\n",
    "os.makedirs('data/03_processed', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# %%\n",
    "# Utility: mock CMAPSS-like dataset generator (if real data is absent)\n",
    "def generate_mock_cmaps_data(n_units=100, max_cycles=200, num_sensors=21):\n",
    "    \"\"\"\n",
    "    Produces a DataFrame similar to CMAPSS. Each unit has repeated cycles.\n",
    "    We'll create smooth degradation signals for some sensors, and noise for others.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for unit in range(1, n_units+1):\n",
    "        # sample unit-specific failure horizon to vary life (between 80 and max_cycles)\n",
    "        horizon = np.random.randint(int(max_cycles*0.4), max_cycles)\n",
    "        for cycle in range(1, horizon+1):\n",
    "            # degrade curve: for a subset of sensors produce monotonic drift\n",
    "            base = cycle / horizon\n",
    "            sensors = []\n",
    "            for s in range(num_sensors):\n",
    "                if s % 3 == 0:\n",
    "                    val = np.clip(base + np.random.normal(0, 0.02), 0, 1.0)  # degradation\n",
    "                elif s % 5 == 0:\n",
    "                    val = np.clip(1 - base + np.random.normal(0, 0.04), 0, 1.0)  # opposite drift\n",
    "                else:\n",
    "                    val = np.random.normal(0.5, 0.1)  # noisy sensor\n",
    "                sensors.append(val)\n",
    "            rows.append([unit, cycle] + sensors)\n",
    "    cols = ['unit_id', 'cycle'] + [f'sensor_{i+1}' for i in range(num_sensors)]\n",
    "    df = pd.DataFrame(rows, columns=cols)\n",
    "    return df\n",
    "\n",
    "# If no raw file present, create mock\n",
    "raw_path = 'data/01_raw/train_FD_mock.txt'\n",
    "if not os.path.exists(raw_path):\n",
    "    print(\"Generating mock CMAPSS-like dataset...\")\n",
    "    df_mock = generate_mock_cmaps_data(n_units=120, max_cycles=250, num_sensors=21)\n",
    "    df_mock.to_csv(raw_path, index=False)\n",
    "    print(f\"Saved mock data to {raw_path}\")\n",
    "\n",
    "# %%\n",
    "# Data preprocessing and sequence creation (adapted from your script, but returns 3D arrays)\n",
    "def load_and_preprocess_for_pipeline(raw_path='data/01_raw/train_FD_mock.txt', sequence_length=SEQUENCE_LENGTH):\n",
    "    df = pd.read_csv(raw_path)\n",
    "    # select sensor columns similar to your earlier selection\n",
    "    # We'll pick 14 sensors indices (1-indexed -> pick sensors 2,3,4,7,8,9,11,12,13,15,17,20,21,14 as earlier)\n",
    "    sensor_indices = [2,3,4,7,8,9,11,12,13,15,17,20,21,14]  # 1-based names\n",
    "    sensor_cols = [f'sensor_{i}' for i in sensor_indices]\n",
    "    # ensure columns exist in the mock\n",
    "    sensor_cols = [c for c in sensor_cols if c in df.columns]\n",
    "    features = ['cycle'] + sensor_cols\n",
    "    df_features = df[['unit_id'] + features].copy()\n",
    "\n",
    "    # scale features per-unit or globally? Use global MinMax for consistency\n",
    "    scaler = MinMaxScaler()\n",
    "    df_features[sensor_cols + ['cycle']] = scaler.fit_transform(df_features[sensor_cols + ['cycle']])\n",
    "\n",
    "    # compute max cycles per unit -> RUL\n",
    "    max_cycles = df_features.groupby('unit_id')['cycle'].max()\n",
    "    def compute_rul(sub):\n",
    "        uc = int(sub.iloc[0]['unit_id'])\n",
    "        maxc = max_cycles.loc[uc]\n",
    "        # but note cycle is scaled; compute RUL on original cycle scale would be better\n",
    "        # we can reconstruct unscaled cycles: but for mock this is ok â€” we'll derive label by position\n",
    "        return None\n",
    "    # Instead, label by distance from end: last 15 cycles per unit are anomalies\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for unit, sub in df_features.groupby('unit_id'):\n",
    "        sub = sub.reset_index(drop=True)\n",
    "        data = sub[features].values\n",
    "        # define last K cycles as failure region\n",
    "        K = 15\n",
    "        n = len(data)\n",
    "        label_vec = np.zeros(n, dtype=int)\n",
    "        label_vec[max(0, n-K):] = 1\n",
    "        # create sliding windows\n",
    "        for i in range(0, n - sequence_length):\n",
    "            seq = data[i:i+sequence_length]  # shape (seq_len, features)\n",
    "            lbl = label_vec[i+sequence_length-1]\n",
    "            sequences.append(seq)\n",
    "            labels.append(lbl)\n",
    "    X = np.stack(sequences, axis=0)   # (N, seq_len, num_features)\n",
    "    y = np.array(labels, dtype=int)\n",
    "    print(f\"Created sequences: X shape {X.shape}, labels {np.bincount(y)}\")\n",
    "    return X, y, scaler\n",
    "\n",
    "# Run preprocessing\n",
    "X_all, y_all, scaler = load_and_preprocess_for_pipeline(raw_path)\n",
    "\n",
    "# save initial arrays in case user wants them\n",
    "np.save('data/02_interim/X_sequences.npy', X_all)\n",
    "np.save('data/02_interim/y_sequences.npy', y_all)\n",
    "\n",
    "# %%\n",
    "# Phase 1: Create DataLoader of NORMAL sequences only and train VAE\n",
    "normal_mask = (y_all == 0)\n",
    "X_normal = X_all[normal_mask]\n",
    "print(f\"Normal sequences for VAE training: {X_normal.shape[0]}\")\n",
    "\n",
    "# Build DataLoader (model expects torch.Tensor batches with shape (batch, seq_len, num_features))\n",
    "tensor_norm = torch.from_numpy(X_normal).float()\n",
    "normal_loader = DataLoader(TensorDataset(tensor_norm), batch_size=128, shuffle=True)\n",
    "\n",
    "# Train VAE (this calls your src.train_vae_from_dataloader implementation)\n",
    "vae_model, history = train_vae_from_dataloader(normal_loader, epochs=40, lr=1e-3, kld_weight=1.0,\n",
    "                                              kl_anneal={'start':0.0, 'end':1.0, 'n_epochs':20},\n",
    "                                              model_savepath='models/vae_normal_manifold_weights.pth',\n",
    "                                              device=DEVICE)\n",
    "\n",
    "# Save history plot\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history['total_loss'], label='total')\n",
    "plt.plot(history['recon_loss'], label='recon')\n",
    "plt.plot(history['kld_loss'], label='kld')\n",
    "plt.legend(); plt.title('VAE training losses'); plt.xlabel('epoch')\n",
    "plt.savefig('reports/vae_training_loss.png', dpi=150); plt.close()\n",
    "\n",
    "# %%\n",
    "# Phase 2a: Synthesis by interpolation (existing method)\n",
    "# Prepare X_all and y_all as required by synthesize_and_save (X_all shape should be (N, seq_len, num_features))\n",
    "synthesize_and_save(vae_model, X_all, y_all, save_dir='data/03_processed', n_per_anomaly=30)\n",
    "\n",
    "# After synthesize_and_save: X_train_combined.npy saved (flattened)\n",
    "X_combined_flat = np.load('data/03_processed/X_train_combined.npy')\n",
    "y_combined = np.load('data/03_processed/y_train_combined.npy')\n",
    "print(\"Combined flattened shapes:\", X_combined_flat.shape, y_combined.shape)\n",
    "\n",
    "# Optionally reshape to sequences for classifier\n",
    "if X_combined_flat.ndim == 2:\n",
    "    X_combined = X_combined_flat.reshape(X_combined_flat.shape[0], SEQUENCE_LENGTH, NUM_FEATURES)\n",
    "else:\n",
    "    X_combined = X_combined_flat\n",
    "print(\"Combined reshaped:\", X_combined.shape, np.bincount(y_combined.astype(int)))\n",
    "\n",
    "# %%\n",
    "# Phase 2b: Additional SMOTE-like latent augmentation\n",
    "def smote_latent_perturbations(model, X_real_anomalies, normal_latent_center, n_augment_per_anom=20, sigma=0.1):\n",
    "    \"\"\"\n",
    "    For each real anomaly latent, sample Gaussian perturbations around it (or around the midpoint to normal center)\n",
    "    to create more realistic variations:\n",
    "      z_base = anomaly_latent\n",
    "      z_aug = z_base + N(0, sigma^2)\n",
    "    Optionally, can sample around (z_base + normal_center)/2 for smoother morphs.\n",
    "    Returns decoded sequences as (M * n_aug, seq_len, num_features)\n",
    "    \"\"\"\n",
    "    # encode anomaly latents\n",
    "    anomaly_latents = encode_dataset_to_latents(model, torch.from_numpy(X_real_anomalies).float().to(DEVICE), batch_size=128, device=DEVICE)\n",
    "    all_aug_latents = []\n",
    "    for z in anomaly_latents:\n",
    "        for _ in range(n_augment_per_anom):\n",
    "            # sample around z with gaussian noise (scale sigma)\n",
    "            z_new = z + np.random.normal(0, sigma, size=z.shape)\n",
    "            all_aug_latents.append(z_new)\n",
    "    all_aug = np.stack(all_aug_latents, axis=0)\n",
    "    # decode\n",
    "    decoded = decode_latents_to_sequences(model, all_aug, batch_size=128, device=DEVICE)\n",
    "    return decoded\n",
    "\n",
    "# prepare real anomaly sequences (3D)\n",
    "anomaly_mask = (y_all == 1)\n",
    "X_real_anoms = X_all[anomaly_mask]\n",
    "print(\"Real anomalies:\", X_real_anoms.shape[0])\n",
    "\n",
    "# compute normal latent center\n",
    "normal_tensor = torch.from_numpy(X_all[normal_mask]).float().to(DEVICE)\n",
    "normal_latents = encode_dataset_to_latents(vae_model, normal_tensor, batch_size=128, device=DEVICE)\n",
    "normal_center = np.mean(normal_latents, axis=0)\n",
    "\n",
    "# generate SMOTE-like augmentations\n",
    "synthetic_smote = smote_latent_perturbations(vae_model, X_real_anoms, normal_center, n_augment_per_anom=25, sigma=0.08)\n",
    "print(\"SMOTE-like synthetic shape:\", synthetic_smote.shape)\n",
    "\n",
    "# flatten and append to combined\n",
    "synth_smote_flat = synthetic_smote.reshape(synthetic_smote.shape[0], -1)\n",
    "X_combined_flat2 = np.concatenate([X_combined_flat, synth_smote_flat], axis=0)\n",
    "y_synth_smote = np.ones(synth_smote_flat.shape[0], dtype=int)\n",
    "y_combined2 = np.concatenate([y_combined, y_synth_smote], axis=0)\n",
    "print(\"New combined shapes:\", X_combined_flat2.shape, y_combined2.shape)\n",
    "\n",
    "# Save extended combined dataset\n",
    "np.save('data/03_processed/X_train_combined_augmented.npy', X_combined_flat2)\n",
    "np.save('data/03_processed/y_train_combined_augmented.npy', y_combined2)\n",
    "\n",
    "# %%\n",
    "# Phase 3: Train LSTM classifier on augmented data\n",
    "# We'll implement training similar to the train_classifier.py logic (but inline here)\n",
    "# Load augmented dataset\n",
    "X_flat = np.load('data/03_processed/X_train_combined_augmented.npy')\n",
    "y = np.load('data/03_processed/y_train_combined_augmented.npy')\n",
    "\n",
    "# reshape to (N, seq_len, num_features)\n",
    "X_seq = X_flat.reshape(-1, SEQUENCE_LENGTH, NUM_FEATURES)\n",
    "\n",
    "# Stratified split for train/val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_seq, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long()),\n",
    "                          batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long()),\n",
    "                        batch_size=128, shuffle=False)\n",
    "\n",
    "# LSTM classifier (small model)\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, num_features=NUM_FEATURES, hidden_size=128, num_layers=1, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=num_features, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out, (h_n, _) = self.lstm(x)\n",
    "        last = h_n[-1]\n",
    "        return self.fc(last)\n",
    "\n",
    "# Instantiate model\n",
    "clf = LSTMClassifier().to(DEVICE)\n",
    "\n",
    "# class weights\n",
    "classes, counts = np.unique(y_train, return_counts=True)\n",
    "inv_freq = 1.0 / counts\n",
    "norm = inv_freq / np.sum(inv_freq) * len(classes)\n",
    "weights = np.zeros(len(classes), dtype=np.float32)\n",
    "for c,w in zip(classes, norm):\n",
    "    weights[c] = w\n",
    "class_weights = torch.tensor(weights, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(clf.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=4, verbose=True)\n",
    "\n",
    "best_f1 = -1.0\n",
    "best_state = None\n",
    "history_clf = {'epoch':[], 'loss':[], 'val_p':[], 'val_r':[], 'val_f1':[]}\n",
    "for epoch in range(1, 31):\n",
    "    clf.train()\n",
    "    running_loss = 0.0\n",
    "    for Xb, yb in train_loader:\n",
    "        Xb = Xb.to(DEVICE); yb = yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = clf(Xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * Xb.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # validation\n",
    "    clf.eval()\n",
    "    all_preds = []; all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for Xv, yv in val_loader:\n",
    "            Xv = Xv.to(DEVICE)\n",
    "            logits = clf(Xv)\n",
    "            preds = torch.softmax(logits, dim=1)[:,1].cpu().numpy()  # anomaly probability\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(yv.numpy())\n",
    "    y_true = np.concatenate(all_labels)\n",
    "    y_score = np.concatenate(all_preds)\n",
    "    # default threshold 0.5\n",
    "    y_pred_bin = (y_score >= 0.5).astype(int)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred_bin, labels=[1], average='binary', zero_division=0)\n",
    "    print(f\"Epoch {epoch} | Loss {epoch_loss:.5f} | Val P {p:.4f} R {r:.4f} F1 {f1:.4f}\")\n",
    "    history_clf['epoch'].append(epoch); history_clf['loss'].append(epoch_loss)\n",
    "    history_clf['val_p'].append(p); history_clf['val_r'].append(r); history_clf['val_f1'].append(f1)\n",
    "\n",
    "    scheduler.step(f1)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_state = clf.state_dict().copy()\n",
    "        torch.save(best_state, 'models/classifier_best_by_f1.pth')\n",
    "        # compute confusion matrix at 0.5 for record\n",
    "        cm = confusion_matrix(y_true, y_pred_bin)\n",
    "        plt.figure(figsize=(4,3)); sns.heatmap(cm, annot=True, fmt='d'); plt.title(f'Best CM Epoch {epoch}'); plt.savefig('reports/confusion_matrix_best.png'); plt.close()\n",
    "        print(f\"Saved best classifier at epoch {epoch} with F1 {f1:.4f}\")\n",
    "\n",
    "# save training history\n",
    "pd.DataFrame(history_clf).to_csv('reports/classifier_history.csv', index=False)\n",
    "\n",
    "# %%\n",
    "# Phase 4: Threshold calibration on validation set (precision-recall curve)\n",
    "# load best model\n",
    "clf.load_state_dict(best_state)\n",
    "clf.to(DEVICE)\n",
    "clf.eval()\n",
    "\n",
    "# get validation scores\n",
    "all_scores = []; all_y = []\n",
    "with torch.no_grad():\n",
    "    for Xv, yv in val_loader:\n",
    "        Xv = Xv.to(DEVICE)\n",
    "        logits = clf(Xv)\n",
    "        probs = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
    "        all_scores.append(probs); all_y.append(yv.numpy())\n",
    "y_val_true = np.concatenate(all_y)\n",
    "y_val_scores = np.concatenate(all_scores)\n",
    "\n",
    "# get precision-recall\n",
    "precision, recall, thresholds = precision_recall_curve(y_val_true, y_val_scores)\n",
    "f1s = (2 * precision * recall) / (precision + recall + 1e-12)\n",
    "best_idx = np.argmax(f1s)\n",
    "best_threshold = 0.5 if thresholds.size == 0 else (thresholds[best_idx] if best_idx < len(thresholds) else 0.5)\n",
    "best_prec = precision[best_idx]; best_rec = recall[best_idx]; best_f1_thr = f1s[best_idx]\n",
    "print(f\"Best threshold by val F1: {best_threshold:.4f} (P={best_prec:.4f}, R={best_rec:.4f}, F1={best_f1_thr:.4f})\")\n",
    "\n",
    "# Plot precision-recall curve and mark chosen threshold\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label='Precision-Recall curve')\n",
    "plt.scatter([best_rec], [best_prec], c='red', label=f'Best F1 @thr={best_threshold:.3f}')\n",
    "plt.xlabel('Recall'); plt.ylabel('Precision'); plt.legend()\n",
    "plt.title('Precision-Recall Curve (Validation)')\n",
    "plt.savefig('reports/precision_recall_curve.png'); plt.close()\n",
    "\n",
    "# Save the chosen threshold to disk for inference\n",
    "with open('models/threshold.txt', 'w') as f:\n",
    "    f.write(str(float(best_threshold)))\n",
    "print(\"Saved threshold to models/threshold.txt\")\n",
    "\n",
    "# %%\n",
    "# Fi\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
